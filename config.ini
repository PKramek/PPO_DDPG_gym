[PPO]
horizon_length = 4000
epochs = 250
gamma = 0.995
epsilon = 0.2
lambda = 0.97
actor_learning_rate = 0.001
critic_learning_rate = 0.001
train_actor_iterations = 110
train_critic_iterations = 110
minibatch_size = 64
hidden_size = 64

[PPO_DoublePrecision]
horizon_length = 4000
epochs = 250
gamma = 0.999
epsilon = 0.2
lambda = 0.97
actor_learning_rate = 0.001
critic_learning_rate = 0.001
train_actor_iterations = 110
train_critic_iterations = 110
minibatch_size = 64
hidden_size = 64

[DDPG]
epochs = 250
horizon_length = 100000
episodes_in_epoch = 4
actor_learning_rate = 0.001
critic_learning_rate = 0.001
start_steps = 10000
action_noise = 0.1
update_after = 1000
update_every = 50
minibatch_size = 100
gamma = 0.99
polyak = 0.995
hidden_size = 256

[DDPG_DoublePrecision]
epochs = 125
horizon_length = 100000
episodes_in_epoch = 4
actor_learning_rate = 0.001
critic_learning_rate = 0.001
start_steps = 10000
action_noise = 0.1
update_after = 2000
update_every = 50
minibatch_size = 100
gamma = 0.994
polyak = 0.995
hidden_size = 256

