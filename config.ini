[PPO]
horizon_length = 4000
epochs = 250
gamma = 0.99
epsilon = 0.2
lambda = 0.95
actor_learning_rate = 0.001
critic_learning_rate = 0.001
train_actor_iterations = 80
train_critic_iterations = 80
minibatch_size = 64
hidden_size = 64

[PPO_DEFAULT]
horizon_length = 4000
epochs = 250
gamma = 0.99
epsilon = 0.2
lambda = 0.95
actor_learning_rate = 0.001
critic_learning_rate = 0.001
train_actor_iterations = 80
train_critic_iterations = 80
minibatch_size = 64
hidden_size = 64

[DDPG]
epochs_num = 10
horizon_len = 100000
episodes_in_epoch = 4
actor_lr = 0.001
critic_lr = 0.001
start_steps = 10000
action_noise = 0.1
max_ep_length = 1000
update_after = 1000
update_every = 50
update_times = 50
batch_size = 100
gamma = 0.99
polyak = 0.995

[DDPG_DEFAULT]
epochs_num = 10
horizon_len = 100000
episodes_in_epoch = 4
actor_lr = 0.001
critic_lr = 0.001
start_steps = 10000
action_noise = 0.1
max_ep_length = 1000
update_after = 1000
update_every = 50
update_times = 50
batch_size = 100
gamma = 0.99
polyak = 0.995

